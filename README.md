# Real-time-Data-Streaming-Application
This repository provides a comprehensive guide to building an end-to-end data engineering pipeline. The project leverages a robust tech stack that includes Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandra, all containerized using Docker for seamless deployment and scalability.

## Key Components:
- Data Source: randomuser.me API to generate random user data.
- Apache Airflow: Orchestrates the pipeline, storing fetched data in PostgreSQL.
- Apache Kafka and Zookeeper: Stream data from PostgreSQL to the processing engine.
- Control Center and Schema Registry: Monitor and manage Kafka streams.
- Apache Spark: Processes data with master and worker nodes.
- Cassandra: Stores the processed data.
## Learning Outcomes:
- Set up a data pipeline with Apache Airflow.
- Implement real-time data streaming with Apache Kafka.
- Achieve distributed synchronization with Apache Zookeeper.
- Apply data processing techniques using Apache Spark.
- Explore data storage solutions with Cassandra and PostgreSQL.
- Containerize the entire data engineering setup using Docker.
## Technologies Used:
- Apache Airflow
- Python
- Apache Kafka
- Apache Zookeeper
- Apache Spark
- Cassandra
- PostgreSQL
- Docker
![Data engineering architecture](https://github.com/user-attachments/assets/741611d8-145a-4f41-a0e1-7c7c49944b9f)
